# Snake Reinforcement Learning

A Snake game implementation trained using Deep Q-Network (DQN) reinforcement learning. The agent learns to play Snake by maximizing rewards through deep neural networks.

## ğŸ¯ What It Does

This project trains an AI agent to play the classic Snake game using reinforcement learning. The agent learns optimal strategies through:
- **DQN (Deep Q-Network)**: Neural network-based Q-learning
- **Experience Replay**: Efficient learning from past experiences
- **Target Network**: Stable training with periodic updates
- **Reward Shaping**: Incentives for approaching food, surviving, and growing

The trained agent navigates a 10x10 grid, avoiding walls and its own body while collecting power-ups to grow longer.

## ğŸ“Š Current Best Scores

**Last 10 episodes:**
- **Average Reward:** 76.39 Â± 361.09
- **Average Steps:** 878.4 Â± 294.2
- **Best Reward:** 263.96

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+ (project uses `snake_rl` environment)
- Node.js (for web interface)

### Installation

1. **Clone the repository:**
```bash
git clone https://github.com/ninja-boldo/snake.git
cd snake
```

2. **Set up Python environment:**
```bash
# Create and activate virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install Python dependencies
pip install -r requirements.txt
```

3. **Install Node.js dependencies (for visualization):**
```bash
npm install
```

### Training the Agent

**Start training from scratch:**
```bash
python rl/train.py
```

**Resume training from saved model:**
```bash
python rl/train.py --load
```

**Training options:**
```bash
python rl/train.py --episodes 20000 --dim 10 --device auto
```

Available options:
- `--episodes N`: Number of training episodes (default: 20000)
- `--dim N`: Board size (NxN grid, default: 10)
- `--device`: Device to use (`auto`, `cpu`, `cuda`, `mps`)
- `--no-amp`: Disable automatic mixed precision (for CUDA)
- `--load`: Load existing model before training

### Evaluating the Agent

**Evaluate trained model:**
```bash
python rl/train.py --eval-only
```

**Evaluate with visualization:**
```bash
python rl/train.py --eval-only --render
```

### Running the Web Interface

```bash
npm run dev
```

Then open your browser to `http://localhost:5173`

## ğŸ“ Project Structure

```
snake/
â”œâ”€â”€ rl/
â”‚   â”œâ”€â”€ train.py           # Main DQN training script
â”‚   â”œâ”€â”€ env_local.py       # Gymnasium environment wrapper
â”‚   â”œâ”€â”€ env_js.py          # JavaScript integration environment
â”‚   â””â”€â”€ train_js.py        # Training with JS visualization
â”œâ”€â”€ game_py/
â”‚   â”œâ”€â”€ snake_engine.py    # Core Snake game logic
â”‚   â””â”€â”€ tests.py           # Game engine tests
â”œâ”€â”€ src/
â”‚   â””â”€â”€ main.ts            # Web interface (TypeScript)
â”œâ”€â”€ public/                # Static web assets
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ package.json           # Node.js dependencies
â”œâ”€â”€ snake_dqn.pth         # Latest trained model
â”œâ”€â”€ snake_dqn_best.pth    # Best performing model
â””â”€â”€ training_results.png  # Training visualization
```

## ğŸ§  Technical Details

### Neural Network Architecture
- **Input:** 19 features (direction encoding, danger detection, food location, snake position)
- **Hidden Layers:** 256 â†’ 256 â†’ 128 neurons with ReLU activation
- **Output:** 4 Q-values (one for each direction: up, right, down, left)

### Training Configuration
- **Learning Rate:** 0.0003
- **Discount Factor (Î³):** 0.95
- **Epsilon Decay:** 1.0 â†’ 0.01 over 5000 episodes
- **Replay Buffer:** 50,000 experiences
- **Batch Size:** 64
- **Target Network Update:** Every 500 steps

### Reward System
- **Power-up collection:** +10.0
- **Moving closer to food:** +0.1
- **Moving away from food:** -0.1
- **Survival:** +0.01 per step
- **Collision (death):** -10.0
- **Near danger:** -0.05

### State Representation (Features)
1. Current direction (one-hot encoded)
2. Danger detection in 4 directions
3. Food direction relative to head
4. Normalized food position
5. Snake length (normalized)
6. Head position (normalized)
7. Tail position (normalized)

## ğŸ® Usage Examples

### Train with custom parameters
```bash
python rl/train.py --episodes 10000 --dim 15 --device cuda
```

### Evaluate multiple episodes
```bash
python rl/train.py --eval-only --render
```

### Use specific features
```bash
python rl/train.py --no-features  # Use raw grid instead of features
```

## ğŸ“ˆ Monitoring Training

During training, the script outputs:
- Episode number and progress
- Average reward over last 100 episodes
- Average episode length
- Training loss
- Current epsilon (exploration rate)

Training plots are automatically saved to `training_results.png` showing:
- Reward progression
- Episode lengths
- Training loss
- Reward distribution

## ğŸ”§ Dependencies

### Python
- PyTorch (deep learning framework)
- Gymnasium (RL environment interface)
- NumPy (numerical computing)
- Matplotlib (visualization)
- Pandas (data analysis)

### JavaScript
- Vite (build tool and dev server)
- TypeScript (type-safe JavaScript)
- WebSocket (real-time communication)

## ğŸ› Troubleshooting

**CUDA out of memory:**
```bash
python rl/train.py --device cpu --no-amp
```

**Model not found:**
Ensure you've trained a model first or specify the correct path to an existing model.

**Web interface not loading:**
Make sure to run `npm install` and check that port 5173 is available.

## ğŸ“ Notes

- Models are automatically saved every 500 episodes
- Best performing model is saved as `snake_dqn_best.pth`
- Training can be interrupted with Ctrl+C and will save progress
- The agent uses epsilon-greedy exploration during training
- Evaluation mode disables exploration for deterministic behavior

## ğŸ¤ Contributing

Feel free to open issues or submit pull requests for improvements!

## ğŸ“„ License

ISC
